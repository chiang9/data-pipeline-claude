# Data Upload Scripts

This directory contains scripts for uploading sample data to MySQL using the data_pipeline module.

## Scripts Overview

### ğŸ“¦ upload_products.py
**Single dataset upload script for products data**

```bash
python scripts/upload_products.py
```

**Features:**
- Uploads `data/products.csv` to MySQL `products` table
- Comprehensive error handling and validation
- Detailed logging and progress reporting
- Prerequisites checking (files, dependencies, database connection)

**Output Example:**
```
ğŸ” Checking prerequisites...
âœ… Prerequisites check passed

=== Products Data Upload Script ===
ğŸ“Š Extract: 20 rows, 17 columns
ğŸ”„ Transform: 20 â†’ 20 rows
ğŸ’¾ Load: 20 rows loaded to 'products'

ğŸ‰ SUCCESS: Products data uploaded successfully!
ğŸ“ Source File: data/products.csv
ğŸ—„ï¸  Database Table: products
ğŸ“Š Records Processed: 20
```

### ğŸ—ƒï¸ upload_all_data.py
**Complete sample data upload script**

```bash
python scripts/upload_all_data.py
```

**Features:**
- Uploads all sample data in correct order (users â†’ products â†’ orders)
- Maintains referential integrity
- Batch processing with individual error handling
- Summary statistics and performance metrics
- Database connection verification

**Upload Order:**
1. **users.csv** â†’ `users` table (20 records)
2. **products.csv** â†’ `products` table (20 records)  
3. **orders.csv** â†’ `orders` table (35 records)

**Output Example:**
```
ğŸ‰ ALL DATA UPLOADED SUCCESSFULLY!
âœ… User Data (20 records)     â†’ users     (20 rows)
âœ… Product Data (20 records)  â†’ products  (20 rows)
âœ… Order Data (35 records)    â†’ orders    (35 rows)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“Š Total Records Uploaded: 75
â±ï¸  Total Time: 2.45 seconds
ğŸ—„ï¸  Database Tables Created: 3
```

## Prerequisites

### 1. Environment Setup
```bash
# Activate virtual environment
source venv_linux/bin/activate

# Install dependencies
pip install -r examples/requirements.txt
```

### 2. Database Configuration
```bash
# Copy environment template
cp examples/.env.example .env

# Edit with your MySQL credentials
DB_HOST=localhost
DB_PORT=3306
DB_USER=your_username
DB_PASSWORD=your_password
DB_NAME=your_database
```

### 3. MySQL Database Setup
```sql
-- Create database
CREATE DATABASE data_pipeline;

-- Create user (optional)
CREATE USER 'pipeline_user'@'localhost' IDENTIFIED BY 'secure_password';
GRANT ALL PRIVILEGES ON data_pipeline.* TO 'pipeline_user'@'localhost';
FLUSH PRIVILEGES;
```

## Usage Patterns

### Basic Upload
```bash
# Upload single dataset
python scripts/upload_products.py

# Upload all datasets
python scripts/upload_all_data.py
```

### With Custom Configuration
```bash
# Set custom database name
DB_NAME=test_pipeline python scripts/upload_all_data.py

# Use different table replacement strategy
# (Edit script to change if_exists parameter)
```

### Development Testing
```bash
# Quick test with products only
python scripts/upload_products.py

# Full integration test
python scripts/upload_all_data.py
```

## Error Handling

### Common Issues

**âŒ Database Connection Failed**
```
ğŸ’¡ Check your .env file and ensure MySQL is running
ğŸ’¡ Verify database credentials and permissions
```

**âŒ Missing CSV Files**
```
ğŸ’¡ Make sure all CSV files exist in the data/ directory
```

**âŒ Import Errors**
```
ğŸ’¡ Activate virtual environment: source venv_linux/bin/activate
ğŸ’¡ Install dependencies: pip install -r examples/requirements.txt
```

### Debug Mode
Add detailed logging for troubleshooting:
```python
import logging
logging.getLogger('data_pipeline').setLevel(logging.DEBUG)
```

## Verification

### Check Upload Success
```sql
-- Verify table creation and row counts
SELECT 
    'users' as table_name, COUNT(*) as row_count FROM users
UNION ALL
SELECT 
    'products' as table_name, COUNT(*) as row_count FROM products  
UNION ALL
SELECT 
    'orders' as table_name, COUNT(*) as row_count FROM orders;
```

### Test Data Relationships
```sql
-- Verify referential integrity
SELECT 
    o.order_id,
    u.first_name,
    u.last_name,
    p.product_name,
    o.total_amount
FROM orders o
JOIN users u ON o.user_id = u.user_id
JOIN products p ON o.product_id = p.product_id
LIMIT 10;
```

### Sample Analytics Queries
```sql
-- Top customers by order value
SELECT 
    u.first_name,
    u.last_name,
    SUM(o.total_amount) as total_spent
FROM users u
JOIN orders o ON u.user_id = o.user_id
GROUP BY u.user_id
ORDER BY total_spent DESC
LIMIT 5;

-- Product sales performance
SELECT 
    p.product_name,
    COUNT(o.order_id) as order_count,
    SUM(o.total_amount) as total_revenue
FROM products p
JOIN orders o ON p.product_id = o.product_id
GROUP BY p.product_id
ORDER BY total_revenue DESC;
```

## Script Architecture

### Data Flow
```
CSV Files â†’ DataPipeline â†’ MySQL Tables
    â†“            â†“            â†“
Validation â†’ Transform â†’ Load & Verify
```

### Error Recovery
- **File validation** before processing
- **Database connectivity** checks
- **Individual dataset** error isolation
- **Detailed logging** for debugging
- **Graceful failure** with helpful messages

### Performance Features
- **Batch processing** for multiple datasets
- **Connection reuse** across uploads
- **Progress reporting** with metrics
- **Optimized table** creation (replace mode)

These scripts provide a complete solution for uploading and testing the sample data with the data pipeline module!